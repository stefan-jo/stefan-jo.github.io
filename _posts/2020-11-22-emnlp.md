# EMNLP 2020

In this blog post I will describe my experience at the [2020 Conference on Empirical Methods in Natural Language Processing](https://2020.emnlp.org/), that took place virtually between Nov 16 and Nov 20. 

It was my first academic ML or NLP conference and I really had a great time. I committed most of my week to the conference and also extended it to the weekend, so that I have two more days to catch up on some papers and talks that I missed during the week. The amount of content that the conference offers in incredible, 752 main conference papers, 25 workshops, 7 tutorials and 5 plenary sessions. Additionally, the conference had plenty of Q&A, gather and demo sessions with paper authors that offer a chance to interact and socialize. 

While the information is still fresh, I decided to write down some of my personal highlights from the conference. Of course this only reflects a small subset of the many possibilities. In my role as self-employed NLP data scientist, which necessarily puts me more in the position of a generalist rather than a deeply specialized researcher, I decided to attend as many plenary sessions, tutorials and workshops as possible to get a broad overview of the field and its progress. Even though I'm still relatively new to the field (with a bit more than two years of working in NLP), I could follow along the highly technical content without any problems. I had a somewhat harder time at Q&A and gather sessions, which require detailed knowledge of each discussed topic as well as having read the paper in advance or at least having watched the pre-recorded paper talk. Next time I participate in a virtual conference, I will definitely take a day or two before it starts to better prepare and select the papers I want to learn more about. 

## Main conference

Of all plenary sessions, which started with a great opening keynote by Claire Cardie from Cornell University on information extraction, my favourite talk was "Friends Donâ€™t Let Friends Deploy Black-Box Models: The Importance of Intelligibility in Machine Learning" by Rich Caruana from Microsoft Research. Even though the talk was actually not about NLP, it was one of the most insightful presentations I experienced at EMNLP. 

In his captivating talk, Rich Caruana made the claim that in the 2020s, so-called *Glass-Box* Machine Learning approaches will take over the world of tabular data. In computer vision and NLP on the other hand, where neural networks most likely will keep the upper hand for quite a while, black-box explainability approaches such as [LIME](https://github.com/marcotcr/lime) and [SHAP](https://github.com/slundberg/shap) are the best way to go for interpretability. But back to tabular data. While tree-based models, such as Random Forests already have some nice interpretability features (e.g. partial dependence), *Glass-Box* ML models go much further. The presented *Explainable Boosting Machines*, or EBMs - part of the family of Generalized Additive Models (GAMs) - are directly interpretable and even editable. Specifically, the functions that EBMs are fitting to single variables or interactions can be inspected visually in the form of graphs. This offers the ability to really explain model behaviour and get valuable insights about the data, which can be invaluable in sensitive domains such as medical applications. Finally, if unwanted patterns are discovered in a graph, the model can be directly edited to avoid making predictions on wrong patterns. All this comes at a comparable performance to gradient-boosting machines or neural networks for tabular data. This could be a real game changer for the world of tabular data. Here is the [repo of InterpretML](https://github.com/interpretml/interpret), which introduces EBMs. 

### Papers

Moving on to EMNLP papers it can be said that Transformer-based models have fully taken over. Among main conference papers, a search for "Transformer" or "BERT" yields 39 and 28 results, respectively, while "LSTM", "RNN" and "CNN" each only return one search result. Because it's absolutely impossible to make the right choice picking the *best* papers from such a huge amount of work, I will just briefly mention a couple of papers that I came across and found particularly interesting. Disclaimer: there is definitely a bias towards dialogue systems, since I recently became interested in that field:
- [Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems](https://www.aclweb.org/anthology/2020.emnlp-main.326/) by Deriu et al. won an Honourable Mention Papers award. They introduce a novel method for evaluating chatbots in which humans have to spot which entity is a bot. 
- [TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue](https://www.aclweb.org/anthology/2020.emnlp-main.66/) by Wu et al. presents a pre-trained BERT model for task-oriented dialogue that sets new SOTA results for intent classification and response ranking. The pre-trained model has also been open-sourced on the huggingface model hub. 
- [Probing Task-Oriented Dialogue Representation from Language Models](https://www.aclweb.org/anthology/2020.emnlp-main.409/) by Wu and Xiong evaluates many language models on various dialogue-related tasks, where ConveRT (see tutorials below), TOD-BERT (above) and TOD-GPT2 achieve the best results.
- [MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems](https://www.aclweb.org/anthology/2020.emnlp-main.273/) by Lin et al. provides a framework for using pre-trained seq2seq models like T5 and BART for dialogue systems especially for low resource settings. 
- [Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start](https://www.aclweb.org/anthology/2020.emnlp-main.660/) by Yin et al. proposes textual entailment as a universal method for NLP when annotations are insufficient. 
- [Active Learning for BERT: An Empirical Study](https://www.aclweb.org/anthology/2020.emnlp-main.638/) by Ein-Dor et al. shows that bringing together transfer learning from pre-trained language models and active learning can help in real-world settings with limited and imbalanced data. 
- [Which BERT? A Survey Organizing Contextualized Encoders](https://www.aclweb.org/anthology/2020.emnlp-main.608/) by Xia, Wu and Van Durme is a much needed survey of different BERT models that helps you choose the right approach. 
- [The Multilingual Amazon Reviews Corpus](https://www.aclweb.org/anthology/2020.emnlp-main.369/) by Keung et al. presents a new large-scale corpus of annotated Amazon reviews in multiple languages along with different benchmarks. 
- [Transformers: State-of-the-Art Natural Language Processing](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) by Wolf et al. presents their amazing huggingface libraries and in my opinion deservedly won Best Demo Paper. 

## Tutorials and workshops

In this section I will briefly describe each of the tutorials and workshops I visited at EMNLP 2020. The bottom line is that I can highly recommend each and every one of these high-quality events. I hope some of the talks will be released to the public in the future. Check the EMNLP and ACL sites for updates. 

### Tutorials

**High Performance Natural Language Processing**

This tutorial by Gabriel Ilharco, Cesar Ilharco, Iulia Turc, Tim Dettmers, Felipe Ferreira and Kenton Lee was very well prepared and attracted quite some positive attention among the NLP research community on twitter. The tutorial focused on a variety of techniques to make state-of-the-art transformer models more efficient. While large-scale pre-trained NLP models enabled remarkable progress on many benchmarks, the size and resource requirements of these models pose serious problems regarding practical usability, costs and accessability. Starting out with some fundamentals to understand attention and transformers, the tutorial continued explaining different techniques such as knowledge distillation, quantization and pruning. It then went on to look at the large number of recent papers aimed at making the attention mechanism more efficient, such as [Big Bird](https://arxiv.org/abs/2007.14062), [Reformer](https://arxiv.org/abs/2001.04451), [Performer](https://arxiv.org/abs/2009.14794) and [Transformer XL](https://arxiv.org/abs/1901.02860). The last part covered the topic of scaling in practice, presenting different opimization techniques and stressing the point that while some optmizations work in theory, in order to really know if they work in practice, you *must* run them on hardware and see what happens. The slides can be found [here](http://gabrielilharco.com/publications/EMNLP_2020_Tutorial__High_Performance_NLP.pdf). 

**The Amazing World of Neural Language Generation**

The second tutorial I visited covered all topics related to language generation with neural networks. Yangfeng Ji first walked us through a variety of neural network models for text generation, from basic RNNs through Autoencoders and GANs to Transformers. The next two modules by Antoine Bosselut covered decoding and training NLG models. While the first part of his talk presented different kinds of decoding algorithms, such as Beam Search, top-k and top-p sampling, the second part looked at different training techniques, including reinforcement learning and human-in-the-loop learning such as a recent [summarization technique from Open AI](https://openai.com/blog/learning-to-summarize-with-human-feedback/). Then, Asli Celikyilmaz covered the important and challenging topic of evaluating neural text generation models. While human evaluation is certainly the gold-standard, it is also very expensive, time-consuming and sometimes inconsistent. Automatic evaluation can be split into untrained metrics, e.g. distance and overlap and trained metrics, e.g. sentence similartiy or learned human feedback. Interesting benchmarking platforms are [ParlAI](https://parl.ai/), which is a open-source platform for dialogue research and [EvalAI](https://eval.ai/), which supports human-in-the-loop and RL environments. In the final part, Thomas Wolf, CSO at huggingface, went into the challenges when deploying large-scale transformer models. He presented several techniques for optimizing encoders, decoders and decoding algorithms in order to speed up inference. [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732) bay Tay et al. (2020) looks at diffent ways to optimize the attention mechanism. The tutorial slides are available [here](https://nlg-world.github.io/).

**Fact-Checking, Fake News, Propaganda, and Media Bias: Truth Seeking in the Post-Truth Era**

In this tutorial, Preslav Nakov and Giovanni Da San Martino cover the important topics of disinformation and propaganda. While the title still includes the term "fake news", Preslav Nakov makes clear early on that we should rather use the term disinformation, which is the overlap of misinformation and malinformation. In other words, disinformation is characterized by falseness and intent to harm. Given the overwhelming amount of news, online discussions and social media posts that are published each day on the internet, using NLP and ML to assist human fact-checkers and/or partially automate the verification process could potentially avoid a lot of harm caused by disinformation campaigns. The tutorial serves both as an introduction to the topic and a presentation of numerous techniques, datasets and challenges. In fact, it covered so much material, that it is impossible to briefly sum up here. But for anyone who is interested in getting started in the field of disinformation, I can only recommend to check the [slides](https://drive.google.com/file/d/1GeTZIFW0QPe6zzYZNGF36aFcX955EzYn/view). 

### Workshops

**SCAI 2: Search-Oriented Conversational AI** 

The second [SCAI](https://scai.info/) workshop, organized by Julia Kiseleva, Jeff Dalton, Aleksandr Chuklin and Mikhail Burtsev, covered talks and papers related to search-oriented conversational AI. The workshop streamed all pre-recorded talks, followed by live Q&A sessions, to simulate the experience of a live conference. My first highlight was a talk titled "Data-Efficient Natural Language Understanding for Task-Oriented Dialogue". In his talk, Ivan VuliÄ‡ from [PolyAI](https://www.polyai.com/) addressed the idea of using pre-trained models that encode conversational knowledge to reduce the need for annotated data for building dialogue systems. [ConveRT](https://www.aclweb.org/anthology/2020.findings-emnlp.196/), which also was a findings paper at EMNLP 2020, uses response selection from large [conversational datasets]((https://github.com/PolyAI-LDN/conversational-datasets)) as pre-training task. The compact Transformer-based model can be fine-tuned on CPUs and achieves great results on response selection and intent classification. My other highlight was [BlenderBot](https://arxiv.org/abs/2004.13637), presented by Jason Weston from Facebook AI Research. The paper presents a recipe for building open-domain chatbots based on large-scale pre-training of Transformer models, fine-tuning and inference optimization. While BlenderBot outperformed Google's [Meena](https://arxiv.org/abs/2001.09977) and achieved near-human performance on some tasks, the authors stress that these results should be seen critically and the model still makes many typical mistakes such as repetition, inconsistencies or factual incorrectness.

**SustaiNLP: Workshop on Simple and Efficient Natural Language Processing**

I've really been looking forward to [SustaiNLP](https://sites.google.com/view/sustainlp2020/home) and it also was one of the main reasons why I considered attending EMNLP this year. Concerned by the complexity and resource requirements of recent large-scale Transformer models, the workshop's goal was to promote "simpler and more sustainable NLP research and practices". My favourite talk at the workshop was "Making Pre-trained Models More Sustainable" by Armand Joulin. The talk starts with the premise that the only thing we know for sure is that we want to simplify pre-trained models - even if we don't know how exactly in advance. Based on this idea, Transformer models can actually be prepared during pre-training to better deal with simplifications during fine-tuning. A way to do this is to apply structured layer dropout during pre-training, which forces the model to spread out its knowledge more evenly across all layers. When the model size is then reduced at the fine-tuning stage, the model already "knows" how to perform well with a subset of its layers. Two papers that I would recommend from the workshop are [SqueezeBERT](https://www.aclweb.org/anthology/2020.sustainlp-1.17/) by Iandola et al., which leverages ideas from computer vision in order to speed up BERT for mobile devices, and [FastFormers](https://www.aclweb.org/anthology/2020.sustainlp-1.20/) by Young Jin Kim and Hany Hassan, which combines distillation, pruning, quantization and runtime optimization to achieve a 233x speedup of BERT-base on CPU at less than a 2% drop in accuracy.

## Concluding thoughts

I still need some time to process all the new information that I got during the last seven days and I have many papers marked that I still want to read, but all in all I can already now say that I learned *a lot*. I can only recommend to participate in conferences like this even if you're (like me) not an experienced NLP researcher. Participating in conferences like EMNLP gives you the unique opportunity to get a detailed impression where the field currently is and look at cutting-edge research from a front row seat. Apart from the already mentioned fact that I want to better prepare in advance for my next conference, my other lesson learned is that I want to set aside more time during the conference to engage socially, make new contacts and exchange ideas.

I also want to say that the concept of a virtual conference really resonates with me. Of course it lacks the personal experience of physically being in a room together with others, but on the other hand it allows people from all over the world to attend top conferences at a much lower cost. I really hope that hybrid-style conferences with both personal attendance and virtual experience will be offered in the future. 
