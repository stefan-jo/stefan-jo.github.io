# EMNLP 2020

!!! WORK IN PROGRESS !!!

In this blog post I will describe my experience at the [2020 Conference on Empirical Methods in Natural Language Processing](https://2020.emnlp.org/), that took place virtually between Nov 16 and Nov 20. 

It was my first academic ML or NLP conference and I really had a great time. I committed most of my week to the conference and also extended it to the weekend, so that I have two more days to catch up on some papers and talks that I missed during the week. The amount of content that the conference offers in incredible, 752 main conference papers, 25 workshops, 7 tutorials and 5 plenary sessions. Additionally, the conference had plenty of Q&A, gather and demo sessions with paper authors that offer a chance to interact and socialize. 

While the information is still fresh, I decided to write down some of my personal highlights from the conference. Of course this only reflects a small subset of the many possibilities. In my role as self-employed NLP data scientist, which necessarily puts me more in the position of a generalist rather than a deeply specialized researcher, I decided to attend as many plenary sessions, tutorials and workshops as possible to get a broad overview of the field and its progress. Even though I'm still relatively new to the field (with a bit more than two years of working in NLP), I could follow along the highly technical content without any problems. I had a somewhat harder time at Q&A and gather sessions, which require detailed knowledge of each discussed topic as well as having read the paper in advance or at least having watched the pre-recorded paper talk. Next time I participate in a virtual conference, I will definitely take a day or two before it starts to better prepare and select the papers I want to learn more about. 

## Main conference

Of all plenary sessions, which started with a great opening keynote by Claire Cardie from Cornell University on information extraction, my favourite talk was "Friends Donâ€™t Let Friends Deploy Black-Box Models: The Importance of Intelligibility in Machine Learning" by Rich Caruana from Microsoft Research. Even though the talk was actually not about NLP, it was one of the most insightful presentations I experienced at EMNLP. 

In his captivating talk, Rich Caruana made the claim that in the 2020s, so-called *Glass-Box* Machine Learning approaches will take over the world of tabular data. In computer vision and NLP on the other hand, where neural networks most likely will keep the upper hand for quite a while, black-box explainability approaches such as [LIME](https://github.com/marcotcr/lime) and [SHAP](https://github.com/slundberg/shap) are the best way to go for interpretability. But back to tabular data. While tree-based models, such as Random Forests already have some nice interpretability features (e.g. partial dependence), *GLass-Box* ML models go much further. The presented *Explainable Boosting Machines*, or EBMs - part of the family of Generalized Additive Models (GAMs) - are directly interpretable and even editable. Specifically, the functions that EBMs are fitting to single variables or interactions can be inspected visually in the form of graphs. This offers the ability to really explain model behaviour and get valuable insights about the data, which can be invaluable in sensitive domains such as medical applications. Finally, if unwanted patterns are discovered in a graph, the model can be directly edited to avoid making predictions on wrong patterns. All this comes at a comparable performance to gradient-boosting machines or neural networks for tabular data. This could be a real game changer for the world of tabular data. Here is the [repo of InterpretML](https://github.com/interpretml/interpret), which introduces EBMs. 

### Papers

Moving on to EMNLP papers it can be said that Transformer-based models have fully taken over. Among main conference papers, a search for "Transformer" or "BERT" yields 39 and 28 results, respectively, while "LSTM", "RNN" and "CNN" each only return one search result. Because it's absolutely impossible to make the right choice picking the *best* papers from such a huge amount of work, I will just briefly mention a couple of papers that I came across and found particularly interesting. Disclaimer: there is definitely a bias towards dialogue systems, since I recently became interested in that field:
- Spot the Bot
- TOD-BERT
- Universal NLP with limited annotations
- Minimalist transfer learning for TOD systems
- [Active Learning for BERT: An Empirical Study](https://www.aclweb.org/anthology/2020.emnlp-main.638/) by Ein-Dor et al. shows that bringing together transfer and active learning can help in real-world settings with limited and imbalanced data. 
- [Which BERT? A Survey Organizing Contextualized Encoders](https://www.aclweb.org/anthology/2020.emnlp-main.608/) by Xia, Wu and Van Durme is a long-needed survey of different BERT models that helps you choose the right model. 
- [The Multilingual Amazon Reviews Corpus](https://www.aclweb.org/anthology/2020.emnlp-main.369/) by Keung et al. presents a new large-scale corpus of annotated Amazon reviews in multiple languages along with different benchmarks. 
- [Transformers: State-of-the-Art Natural Language Processing](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) by Wolf et al. presents their amazing huggingface libraries and in my opinion deservedly won Best Demo Paper. 

## Workshops and Tutorials

!!! WORK IN PROGRESS !!!

### Workshops

**SCAI**

**SustaiNLP**

### Tutorials

**High-performance NLP**

**The amazing world of NLG**

**Fact-Checking, Fake News, Propaganda, and Media Bias: Truth Seeking in the Post-Truth Era**

## Concluding thoughts

I still need some time to process all the new information that I got during the last seven days and I have many papers marked that I still want to read, but all in all I can already now say that I learned *a lot*. I can only recommend to participate in conferences like this even if you're (like me) not an experienced NLP researcher. Participating in conferences like EMNLP gives you the unique opportunity to get a detailed impression where the field currently is and look at cutting-edge research from a front row seat. Apart from the already mentioned fact that I want to better prepare in advance for my next conference, my other lesson learned is that I want to set aside more time during the conference to engage socially, make new contacts and exchange ideas.

I also want to say that the concept of a virtual conference really resonates with me. Of course it lacks the personal experience of physically being in a room together with others, but on the other hand it allows people from all over the world to attend top conferences at a much lower cost. I really hope that hybrid-style conferences with both personal attendance and virtual experience will be offered in the future. 
